{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "431c4e2e",
   "metadata": {},
   "source": [
    "# Implemente o algoritmo KNN, Naive Bayes e Hunt e aplique no dataset IRIS:\n",
    "https://www.kaggle.com/uciml/iris\n",
    "Não use bibliotecas prontas, implemente toda a lógica do algoritmo.\n",
    "Separe aleatoriamente 70% dos dados para treino e 30% para teste e reporte com um print da saída qual a acurácia do algoritmo (número de acertos).\n",
    "\n",
    "Aluno: Vitor Albuquerque de Paula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bd0468",
   "metadata": {},
   "source": [
    "## Pré processamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19c1a680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "url = 'https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv'\n",
    "iris_data = pd.read_csv(url)\n",
    "\n",
    "train_data, test_data = train_test_split(iris_data, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cc2776",
   "metadata": {},
   "source": [
    "## Algoritmo KNN no conjunto de dados Iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f18f02ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do KNN (k=5): 100.0%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# Função para calcular a distância euclidiana\n",
    "def euclidean_distance(x, y):\n",
    "    return np.sqrt(np.sum((x - y) ** 2))\n",
    "\n",
    "# Função para retornar os 'k' vizinhos mais próximos\n",
    "def get_k_neighbors(k, train, test_sample):\n",
    "    distances = []\n",
    "    for i in range(len(train)):\n",
    "        distances.append((i, euclidean_distance(train.iloc[i, :-1], test_sample)))\n",
    "    distances.sort(key=lambda x: x[1])\n",
    "    neighbors = [train.iloc[x[0], -1] for x in distances[:k]]\n",
    "    return neighbors\n",
    "\n",
    "# Função para fazer previsões usando o KNN\n",
    "def knn_predict(k, train, test):\n",
    "    predictions = []\n",
    "    for _, test_sample in test.iterrows():\n",
    "        neighbors = get_k_neighbors(k, train, test_sample[:-1])\n",
    "        majority_class = Counter(neighbors).most_common(1)[0][0]\n",
    "        predictions.append(majority_class)\n",
    "    return predictions\n",
    "\n",
    "# Função para calcular a acurácia das previsões\n",
    "def accuracy(y_true, y_pred):\n",
    "    return round(sum([yt == yp for yt, yp in zip(y_true, y_pred)]) / len(y_true) * 100, 2)\n",
    "\n",
    "url = 'https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv'\n",
    "iris_data = pd.read_csv(url)\n",
    "\n",
    "train_data, test_data = train_test_split(iris_data, test_size=0.3, random_state=42)\n",
    "\n",
    "k = 5  # Por exemplo, escolhendo k=5\n",
    "predictions = knn_predict(k, train_data, test_data)\n",
    "accuracy = accuracy(test_data.iloc[:, -1], predictions)\n",
    "\n",
    "print(f\"Acurácia do KNN (k={k}): {accuracy}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d267c3",
   "metadata": {},
   "source": [
    "## Algoritmo Naive Bayes no conjunto de dados Iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "664790e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do Naive Bayes Gaussiano: 97.78%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from math import exp, sqrt, pi, log\n",
    "\n",
    "# Função para calcular a função PDF (Probability Density Function - Função Densidade de Probabilidade) Gaussiana dado x, média e variação\n",
    "def gauss_pdf(x, mean, var):\n",
    "    const = sqrt(2 * pi * var)\n",
    "    exponent = exp(-(x - mean) ** 2 / (2 * var))\n",
    "    return exponent / const\n",
    "\n",
    "# Função renomeada 'accuracy()' para 'get_accuracy()'\n",
    "def get_accuracy(y_true, y_pred):\n",
    "    return round(sum([yt == yp for yt, yp in zip(y_true, y_pred)]) / len(y_true) * 100, 2)\n",
    "\n",
    "# Função principal para treinar e prever usando Naive Bayes Gaussiano\n",
    "def naive_bayes(train, test, epsilon=1e-9):\n",
    "    X_train = train.iloc[:, :-1]  # Divide as características do conjunto de treinamento\n",
    "    y_train = train.iloc[:, -1]  # Extrai os rótulos do conjunto de treinamento\n",
    "    X_test = test.iloc[:, :-1]  # Divide as características do conjunto de teste\n",
    "    y_test = test.iloc[:, -1]  # Extrai os rótulos do conjunto de teste\n",
    "\n",
    "    # Codifica os rótulos das classes em valores numéricos\n",
    "    label_encoder = LabelEncoder().fit(y_train)\n",
    "    y_train_encoded = label_encoder.transform(y_train)\n",
    "    label_classes = label_encoder.classes_\n",
    "\n",
    "    num_features = X_train.shape[1]\n",
    "\n",
    "    test_class_probs = []\n",
    "    for index, test_sample in X_test.iterrows():\n",
    "        class_probs = []\n",
    "        for label in label_classes:\n",
    "            class_prob = log(get_class_prob(y_train_encoded, label) + epsilon)  # Acrescenta epsilon para evitar erro de log(0)\n",
    "\n",
    "            conditional_probs = []\n",
    "            for feature_idx, feature in enumerate(test_sample):\n",
    "                feature_values = X_train[y_train == label].iloc[:, feature_idx]\n",
    "                mean, var = feature_values.mean(), feature_values.var()\n",
    "\n",
    "                # Calcula a probabilidade de uma amostra pertencer a uma classe \n",
    "                # com base na distribuição Gaussiana das características\n",
    "                likelihood = gauss_pdf(feature, mean, var)\n",
    "                conditional_probs.append(log(likelihood + epsilon))  # Acrescenta epsilon para evitar erro de log(0)\n",
    "            \n",
    "            class_probs.append(class_prob + sum(conditional_probs))\n",
    "\n",
    "        test_class_probs.append(np.argmax(class_probs))\n",
    "\n",
    "    return label_encoder.inverse_transform(test_class_probs)\n",
    "\n",
    "url = 'https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv'\n",
    "iris_data = pd.read_csv(url)\n",
    "\n",
    "train_data, test_data = train_test_split(iris_data, test_size=0.3, random_state=42)\n",
    "\n",
    "predictions = naive_bayes(train_data, test_data)\n",
    "acc = get_accuracy(test_data.iloc[:, -1], predictions)  # Altera 'accuracy' para 'get_accuracy'\n",
    "\n",
    "print(f\"Acurácia do Naive Bayes Gaussiano: {acc}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56c9047",
   "metadata": {},
   "source": [
    "## Algoritmo Hunt no conjunto de dados Iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a82378ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia da Árvore de Decisão (algoritmo de Hunt): 71.11%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "# Classe para armazenar os nós da árvore de decisão\n",
    "class TreeNode:\n",
    "    def __init__(self, feature=None, value=None, class_label=None, left=None, right=None):\n",
    "        self.feature = feature  # Característica usada para dividir o nó\n",
    "        self.value = value  # Valor associado à característica no nó\n",
    "        self.class_label = class_label  # Rótulo da classe para nó folha\n",
    "        self.left = left  # Filho à esquerda da árvore\n",
    "        self.right = right  # Filho à direita da árvore\n",
    "\n",
    "# A função de recorrência principal para construir a árvore de decisão usando o algoritmo de Hunt\n",
    "def hunt_algorithm(data, features):\n",
    "    # Se todas as amostras no conjunto de dados têm a mesma classe, retorne um nó folha com esse rótulo de classe \n",
    "    if len(data['variety'].unique()) == 1:\n",
    "        return TreeNode(class_label=data['variety'].unique()[0])\n",
    "\n",
    "    # Se não há mais recursos para dividir, retorne um nó folha com a classe mais frequente no conjunto de dados\n",
    "    if len(features) == 0:\n",
    "        return TreeNode(class_label=data['variety'].value_counts().idxmax())\n",
    "\n",
    "    # Encontre o melhor recurso para dividir o conjunto de dados atual\n",
    "    best_feature = None\n",
    "    best_gain_ratio = -1\n",
    "    S = entropy(data)\n",
    "\n",
    "    for feature in features:\n",
    "        gain, split_info = info_gain(data, feature, S)\n",
    "        gain_ratio = gain / split_info if split_info != 0 else gain\n",
    "\n",
    "        if gain_ratio > best_gain_ratio:\n",
    "            best_gain_ratio = gain_ratio\n",
    "            best_feature = feature\n",
    "\n",
    "    # Dividir o conjunto de dados no melhor recurso\n",
    "    left_data = data[data[best_feature] == 0].drop(columns=[best_feature])\n",
    "    right_data = data[data[best_feature] == 1].drop(columns=[best_feature])\n",
    "    remaining_features = [f for f in features if f != best_feature]\n",
    "\n",
    "    # Se algum dos subconjuntos resultantes for vazio, retorne um nó folha com a classe mais frequente no conjunto de dados\n",
    "    if left_data.empty or right_data.empty:\n",
    "        return TreeNode(class_label=data['variety'].value_counts().idxmax())\n",
    "\n",
    "    # Construa a subárvore esquerda e direita recursivamente e retorne o nó raiz atual\n",
    "    left_child = hunt_algorithm(left_data, remaining_features)\n",
    "    right_child = hunt_algorithm(right_data, remaining_features)\n",
    "\n",
    "    return TreeNode(feature=best_feature, left=left_child, right=right_child)\n",
    "\n",
    "# Função para medir a impureza do conjunto de dados (entropia)\n",
    "def entropy(data):\n",
    "    prob = data['variety'].value_counts(normalize=True)  # Mudamos 'class' para 'variety'\n",
    "    return -np.sum(prob * np.log2(prob))\n",
    "\n",
    "# Função para calcular o ganho de informação ao dividir um conjunto de dados em um determinado recurso\n",
    "def info_gain(data, feature, S):\n",
    "    prob_split = data[feature].value_counts(normalize=True)\n",
    "    entropy_after_split = -np.sum(prob_split * np.log2(data.groupby(feature)['variety'].value_counts(normalize=True)))  # Mudamos 'class' para 'variety'\n",
    "    gain = S - entropy_after_split\n",
    "    split_info = -np.sum(prob_split * np.log2(prob_split))\n",
    "    return gain, split_info\n",
    "\n",
    "# Função para prever a classe de uma amostra com base na árvore de decisão\n",
    "def predict_sample(tree, sample):\n",
    "    if tree.class_label is not None:\n",
    "        return tree.class_label\n",
    "    feature_value = sample[tree.feature]\n",
    "    return predict_sample(tree.left if feature_value == 0 else tree.right, sample)\n",
    "\n",
    "# Função para prever as classes de um conjunto de teste usando a árvore de decisão\n",
    "def hunt_predict(tree, test_data):\n",
    "    return [predict_sample(tree, sample) for _, sample in test_data.iterrows()]\n",
    "\n",
    "url = 'https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv'\n",
    "iris_data = pd.read_csv(url)\n",
    "\n",
    "train_data, test_data = train_test_split(iris_data, test_size=0.3, random_state=42)\n",
    "\n",
    "# Discretize Iris features\n",
    "# Decidi discretizar as características do conjunto de dados Iris porque o algoritmo Hunt funciona melhor\n",
    "# com características categóricas. \n",
    "# A discretização é feita dividindo os valores numéricos das características em intervalos, convertendo-os\n",
    "# em valores categóricos.\n",
    "binning = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform', subsample=200_000)\n",
    "train_data_binned = train_data.copy()\n",
    "test_data_binned = test_data.copy()\n",
    "column_names = train_data_binned.columns[:-1]\n",
    "\n",
    "train_data_binned[column_names] = binning.fit_transform(train_data_binned[column_names])\n",
    "test_data_binned[column_names] = binning.transform(test_data_binned[column_names])\n",
    "\n",
    "columns = train_data_binned.columns[:-1]\n",
    "decision_tree = hunt_algorithm(train_data_binned, columns)\n",
    "predictions = hunt_predict(decision_tree, test_data_binned)\n",
    "\n",
    "acc = get_accuracy(test_data.iloc[:, -1], predictions)\n",
    "\n",
    "print(f\"Acurácia da Árvore de Decisão (algoritmo de Hunt): {acc}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
