{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3430960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 8.17860964\n",
      "Iteration 2, loss = 3.93276330\n",
      "Iteration 3, loss = 2.89211569\n",
      "Iteration 4, loss = 2.69002341\n",
      "Iteration 5, loss = 2.24377545\n",
      "Iteration 6, loss = 2.11038617\n",
      "Iteration 7, loss = 1.76260473\n",
      "Iteration 8, loss = 1.51010108\n",
      "Iteration 9, loss = 1.70553435\n",
      "Iteration 10, loss = 1.48484512\n",
      "Iteration 11, loss = 1.39051477\n",
      "Iteration 12, loss = 1.23407094\n",
      "Iteration 13, loss = 1.26913074\n",
      "Iteration 14, loss = 1.22338059\n",
      "Iteration 15, loss = 1.00848038\n",
      "Iteration 16, loss = 1.06672144\n",
      "Iteration 17, loss = 1.10094339\n",
      "Iteration 18, loss = 1.02321684\n",
      "Iteration 19, loss = 1.03388975\n",
      "Iteration 20, loss = 0.80121720\n",
      "Iteration 21, loss = 0.95098582\n",
      "Iteration 22, loss = 0.91058440\n",
      "Iteration 23, loss = 0.85233369\n",
      "Iteration 24, loss = 0.74177833\n",
      "Iteration 25, loss = 0.90731973\n",
      "Iteration 26, loss = 0.83734189\n",
      "Iteration 27, loss = 0.73602089\n",
      "Iteration 28, loss = 0.76930757\n",
      "Iteration 29, loss = 0.79521493\n",
      "Iteration 30, loss = 0.82878300\n",
      "Iteration 31, loss = 0.80829712\n",
      "Iteration 32, loss = 0.73712587\n",
      "Iteration 33, loss = 0.74024426\n",
      "Iteration 34, loss = 0.70729392\n",
      "Iteration 35, loss = 0.76976647\n",
      "Iteration 36, loss = 0.60913144\n",
      "Iteration 37, loss = 0.77071560\n",
      "Iteration 38, loss = 0.67867353\n",
      "Iteration 39, loss = 0.55037110\n",
      "Iteration 40, loss = 0.61023251\n",
      "Iteration 41, loss = 0.66807758\n",
      "Iteration 42, loss = 0.68867651\n",
      "Iteration 43, loss = 0.60717590\n",
      "Iteration 44, loss = 0.57437365\n",
      "Iteration 45, loss = 0.60943916\n",
      "Iteration 46, loss = 0.66002945\n",
      "Iteration 47, loss = 0.53696975\n",
      "Iteration 48, loss = 0.61561653\n",
      "Iteration 49, loss = 0.52618879\n",
      "Iteration 50, loss = 0.47691139\n",
      "Iteration 51, loss = 0.60335787\n",
      "Iteration 52, loss = 0.59722278\n",
      "Iteration 53, loss = 0.54236054\n",
      "Iteration 54, loss = 0.51427665\n",
      "Iteration 55, loss = 0.55404508\n",
      "Iteration 56, loss = 0.47655590\n",
      "Iteration 57, loss = 0.45465260\n",
      "Iteration 58, loss = 0.55665684\n",
      "Iteration 59, loss = 0.54301125\n",
      "Iteration 60, loss = 0.54994859\n",
      "Iteration 61, loss = 0.44414743\n",
      "Iteration 62, loss = 0.50331598\n",
      "Iteration 63, loss = 0.49544848\n",
      "Iteration 64, loss = 0.45601805\n",
      "Iteration 65, loss = 0.52124822\n",
      "Iteration 66, loss = 0.49502683\n",
      "Iteration 67, loss = 0.47510025\n",
      "Iteration 68, loss = 0.41873020\n",
      "Iteration 69, loss = 0.48486643\n",
      "Iteration 70, loss = 0.46374670\n",
      "Iteration 71, loss = 0.45751779\n",
      "Iteration 72, loss = 0.44636162\n",
      "Iteration 73, loss = 0.43446554\n",
      "Iteration 74, loss = 0.47229656\n",
      "Iteration 75, loss = 0.43114355\n",
      "Iteration 76, loss = 0.41204853\n",
      "Iteration 77, loss = 0.46653841\n",
      "Iteration 78, loss = 0.36907736\n",
      "Iteration 79, loss = 0.45522479\n",
      "Iteration 80, loss = 0.45105353\n",
      "Iteration 81, loss = 0.36103685\n",
      "Iteration 82, loss = 0.44588355\n",
      "Iteration 83, loss = 0.38856154\n",
      "Iteration 84, loss = 0.37285506\n",
      "Iteration 85, loss = 0.41019799\n",
      "Iteration 86, loss = 0.35160119\n",
      "Iteration 87, loss = 0.32775320\n",
      "Iteration 88, loss = 0.43329606\n",
      "Iteration 89, loss = 0.35701658\n",
      "Iteration 90, loss = 0.33323335\n",
      "Iteration 91, loss = 0.35738622\n",
      "Iteration 92, loss = 0.35686672\n",
      "Iteration 93, loss = 0.38349868\n",
      "Iteration 94, loss = 0.34183463\n",
      "Iteration 95, loss = 0.36082868\n",
      "Iteration 96, loss = 0.34355009\n",
      "Iteration 97, loss = 0.29790338\n",
      "Iteration 98, loss = 0.32771509\n",
      "Iteration 99, loss = 0.32972461\n",
      "Iteration 100, loss = 0.32740195\n",
      "Iteration 101, loss = 0.28245210\n",
      "Iteration 102, loss = 0.25742516\n",
      "Iteration 103, loss = 0.30156855\n",
      "Iteration 104, loss = 0.26974635\n",
      "Iteration 105, loss = 0.29773165\n",
      "Iteration 106, loss = 0.21591386\n",
      "Iteration 107, loss = 0.25514110\n",
      "Iteration 108, loss = 0.23879181\n",
      "Iteration 109, loss = 0.27570496\n",
      "Iteration 110, loss = 0.25805027\n",
      "Iteration 111, loss = 0.22944063\n",
      "Iteration 112, loss = 0.24804231\n",
      "Iteration 113, loss = 0.19542321\n",
      "Iteration 114, loss = 0.22280856\n",
      "Iteration 115, loss = 0.21100203\n",
      "Iteration 116, loss = 0.15995286\n",
      "Iteration 117, loss = 0.18445346\n",
      "Iteration 118, loss = 0.13266273\n",
      "Iteration 119, loss = 0.13810807\n",
      "Iteration 120, loss = 0.11849929\n",
      "Iteration 121, loss = 0.10829896\n",
      "Iteration 122, loss = 0.12116560\n",
      "Iteration 123, loss = 0.10332910\n",
      "Iteration 124, loss = 0.09213649\n",
      "Iteration 125, loss = 0.08543769\n",
      "Iteration 126, loss = 0.09184427\n",
      "Iteration 127, loss = 0.07635648\n",
      "Iteration 128, loss = 0.05944847\n",
      "Iteration 129, loss = 0.06018904\n",
      "Iteration 130, loss = 0.04210781\n",
      "Iteration 131, loss = 0.04502297\n",
      "Iteration 132, loss = 0.04580151\n",
      "Iteration 133, loss = 0.02738029\n",
      "Iteration 134, loss = 0.03049140\n",
      "Iteration 135, loss = 0.02708917\n",
      "Iteration 136, loss = 0.03079064\n",
      "Iteration 137, loss = 0.03001141\n",
      "Iteration 138, loss = 0.02228630\n",
      "Iteration 139, loss = 0.02560078\n",
      "Iteration 140, loss = 0.02223479\n",
      "Iteration 141, loss = 0.02962609\n",
      "Iteration 142, loss = 0.01999758\n",
      "Iteration 143, loss = 0.02131705\n",
      "Iteration 144, loss = 0.02043621\n",
      "Iteration 145, loss = 0.02205774\n",
      "Iteration 146, loss = 0.02040044\n",
      "Iteration 147, loss = 0.02864937\n",
      "Iteration 148, loss = 0.02863156\n",
      "Iteration 149, loss = 0.01816506\n",
      "Iteration 150, loss = 0.02468178\n",
      "Iteration 151, loss = 0.05120607\n",
      "Iteration 152, loss = 0.01929632\n",
      "Iteration 153, loss = 0.01622870\n",
      "Iteration 154, loss = 0.01794570\n",
      "Iteration 155, loss = 0.01750662\n",
      "Iteration 156, loss = 0.02022945\n",
      "Iteration 157, loss = 0.01741352\n",
      "Iteration 158, loss = 0.02707044\n",
      "Iteration 159, loss = 0.01497153\n",
      "Iteration 160, loss = 0.02119209\n",
      "Iteration 161, loss = 0.01762310\n",
      "Iteration 162, loss = 0.03320005\n",
      "Iteration 163, loss = 0.02039512\n",
      "Iteration 164, loss = 0.01667844\n",
      "Iteration 165, loss = 0.01804522\n",
      "Iteration 166, loss = 0.01877829\n",
      "Iteration 167, loss = 0.01761939\n",
      "Iteration 168, loss = 0.01996700\n",
      "Iteration 169, loss = 0.01712996\n",
      "Iteration 170, loss = 0.03959172\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[[85125    24]\n",
      " [  243 85197]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85149\n",
      "           1       1.00      1.00      1.00     85440\n",
      "\n",
      "    accuracy                           1.00    170589\n",
      "   macro avg       1.00      1.00      1.00    170589\n",
      "weighted avg       1.00      1.00      1.00    170589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando bibliotecas\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Carregando os dados\n",
    "df = pd.read_csv('creditcard_2023.csv')\n",
    "\n",
    "# Separando recursos e rótulos\n",
    "X = df.drop(['Class'], axis=1)  # Recursos\n",
    "y = df['Class']  # Rótulo\n",
    "\n",
    "# Dividindo os dados em conjuntos de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Construindo e treinando o modelo MLP com saída detalhada\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, verbose=True, random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Fazendo previsões e avaliando o modelo\n",
    "y_pred = mlp.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185ec3ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
